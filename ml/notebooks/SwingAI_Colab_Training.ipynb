{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ SwingAI Training Notebook V2\n",
        "\n",
        "**AI-Powered Swing Trading for Indian Markets**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Overview\n",
        "\n",
        "This notebook trains the SwingAI ensemble model:\n",
        "- **CatBoost** (35%) - Gradient boosting for tabular data\n",
        "- **TFT** (35%) - Temporal Fusion Transformer for sequences\n",
        "- **Stockformer** (30%) - STL-inspired transformer\n",
        "\n",
        "## üéØ Features\n",
        "- **40 Pure OHLCV Features** (Price Action, SMC/ICT, Volume, MTF)\n",
        "- **No External Dependencies** - Just stock price data\n",
        "- **Rule-Based Filters** - VIX, FII/DII applied separately\n",
        "\n",
        "## ‚öôÔ∏è Requirements\n",
        "- Google Colab with GPU (T4 recommended)\n",
        "- ~2 hours training time\n",
        "- ~5GB RAM\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ How to Use\n",
        "\n",
        "1. Open in Google Colab\n",
        "2. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
        "3. Run all cells in order\n",
        "4. Models will be saved to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install Dependencies\n",
        "%pip install -q catboost==1.2.2 yfinance==0.2.36 pandas numpy scikit-learn torch\n",
        "\n",
        "import torch\n",
        "print(f\"‚úÖ GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 2: Imports & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import yfinance as yf\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "@dataclass\n",
        "class Config:\n",
        "    LOOKBACK_DAYS: int = 60\n",
        "    PREDICTION_HORIZON: int = 5\n",
        "    MIN_HISTORY_DAYS: int = 252\n",
        "    UP_THRESHOLD: float = 0.03\n",
        "    DOWN_THRESHOLD: float = -0.02\n",
        "    TRAIN_END: str = \"2024-06-30\"\n",
        "    VAL_END: str = \"2024-09-30\"\n",
        "    CATBOOST_WEIGHT: float = 0.35\n",
        "    TFT_WEIGHT: float = 0.35\n",
        "    STOCKFORMER_WEIGHT: float = 0.30\n",
        "    BATCH_SIZE: int = 64\n",
        "    LEARNING_RATE: float = 1e-3\n",
        "    MAX_EPOCHS: int = 50\n",
        "    PATIENCE: int = 5\n",
        "    CATBOOST_ITERATIONS: int = 1000\n",
        "    CATBOOST_DEPTH: int = 6\n",
        "    CATBOOST_LR: float = 0.05\n",
        "    TFT_HIDDEN_SIZE: int = 64\n",
        "    TFT_ATTENTION_HEADS: int = 4\n",
        "    TFT_DROPOUT: float = 0.1\n",
        "    STOCKFORMER_D_MODEL: int = 64\n",
        "    STOCKFORMER_N_HEADS: int = 4\n",
        "    STOCKFORMER_N_LAYERS: int = 2\n",
        "    NUM_FEATURES: int = 40\n",
        "    MODEL_SAVE_PATH: str = \"/content/drive/MyDrive/SwingAI/models/\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# 40 Features\n",
        "FEATURE_NAMES = [\n",
        "    'return_1d', 'return_5d', 'return_10d', 'return_20d', 'volatility_20d',\n",
        "    'close_to_sma_20', 'close_to_sma_50', 'rsi_14_norm', 'macd_histogram_norm', 'bb_position',\n",
        "    'structure_score', 'range_position', 'dist_to_swing_high', 'dist_to_swing_low',\n",
        "    'in_discount', 'in_deep_discount', 'in_premium', 'near_bullish_ob', 'near_bearish_ob',\n",
        "    'bullish_fvg', 'bearish_fvg', 'sweep_high', 'sweep_low', 'bos_bullish', 'bos_bearish',\n",
        "    'volume_ratio', 'volume_trend', 'obv_slope', 'close_to_vwap',\n",
        "    'buying_pressure', 'accumulation_score', 'big_volume_day', 'higher_high',\n",
        "    'daily_trend', 'weekly_trend', 'monthly_trend', 'mtf_alignment',\n",
        "    'weekly_range_pos', 'monthly_range_pos', 'trend_strength',\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Config loaded: {len(FEATURE_NAMES)} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Step 3: Stock Universe & Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# F&O Stocks (liquid + can short)\n",
        "FO_STOCKS = [\n",
        "    \"RELIANCE.NS\", \"TCS.NS\", \"HDFCBANK.NS\", \"INFY.NS\", \"ICICIBANK.NS\",\n",
        "    \"HINDUNILVR.NS\", \"SBIN.NS\", \"BHARTIARTL.NS\", \"KOTAKBANK.NS\", \"ITC.NS\",\n",
        "    \"LT.NS\", \"AXISBANK.NS\", \"ASIANPAINT.NS\", \"MARUTI.NS\", \"HCLTECH.NS\",\n",
        "    \"SUNPHARMA.NS\", \"TITAN.NS\", \"BAJFINANCE.NS\", \"ULTRACEMCO.NS\", \"NTPC.NS\",\n",
        "    \"WIPRO.NS\", \"NESTLEIND.NS\", \"POWERGRID.NS\", \"M&M.NS\", \"TATAMOTORS.NS\",\n",
        "    \"JSWSTEEL.NS\", \"ADANIENT.NS\", \"ADANIPORTS.NS\", \"TATASTEEL.NS\", \"ONGC.NS\",\n",
        "    \"TECHM.NS\", \"HDFCLIFE.NS\", \"DIVISLAB.NS\", \"BAJAJFINSV.NS\", \"GRASIM.NS\",\n",
        "    \"DRREDDY.NS\", \"CIPLA.NS\", \"BRITANNIA.NS\", \"EICHERMOT.NS\", \"APOLLOHOSP.NS\",\n",
        "    \"COALINDIA.NS\", \"SBILIFE.NS\", \"BPCL.NS\", \"INDUSINDBK.NS\", \"TATACONSUM.NS\",\n",
        "    \"HEROMOTOCO.NS\", \"HINDALCO.NS\", \"BAJAJ-AUTO.NS\", \"LTIM.NS\", \"SHRIRAMFIN.NS\",\n",
        "    \"TRENT.NS\", \"POLYCAB.NS\", \"PERSISTENT.NS\", \"DIXON.NS\", \"TATAELXSI.NS\",\n",
        "    \"ABB.NS\", \"SIEMENS.NS\", \"HAL.NS\", \"BEL.NS\", \"IRCTC.NS\",\n",
        "    \"ZOMATO.NS\", \"COFORGE.NS\", \"MUTHOOTFIN.NS\", \"INDHOTEL.NS\", \"BANKBARODA.NS\",\n",
        "    \"PNB.NS\", \"IDFCFIRSTB.NS\", \"FEDERALBNK.NS\", \"CHOLAFIN.NS\", \"VEDL.NS\",\n",
        "]\n",
        "\n",
        "def download_stock(symbol, start, end):\n",
        "    try:\n",
        "        df = yf.download(symbol, start=start, end=end, progress=False)\n",
        "        if len(df) < config.MIN_HISTORY_DAYS:\n",
        "            return None\n",
        "        df['Symbol'] = symbol.replace('.NS', '')\n",
        "        return df\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def download_all_stocks(symbols, start, end):\n",
        "    data = {}\n",
        "    print(f\"üì• Downloading {len(symbols)} stocks...\")\n",
        "    \n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(download_stock, sym, start, end): sym for sym in symbols}\n",
        "        for i, future in enumerate(as_completed(futures)):\n",
        "            symbol = futures[future]\n",
        "            df = future.result()\n",
        "            if df is not None:\n",
        "                data[symbol] = df\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"   Progress: {i+1}/{len(symbols)}\")\n",
        "    \n",
        "    print(f\"‚úÖ Downloaded {len(data)} stocks\")\n",
        "    return data\n",
        "\n",
        "# Download 5 years of data\n",
        "stock_data = download_all_stocks(FO_STOCKS, \"2019-01-01\", \"2024-12-31\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def calculate_features(df):\n",
        "    \"\"\"Calculate all 40 features from OHLCV data\"\"\"\n",
        "    data = df.copy()\n",
        "    \n",
        "    # === PRICE ACTION (10) ===\n",
        "    data['return_1d'] = data['Close'].pct_change(1)\n",
        "    data['return_5d'] = data['Close'].pct_change(5)\n",
        "    data['return_10d'] = data['Close'].pct_change(10)\n",
        "    data['return_20d'] = data['Close'].pct_change(20)\n",
        "    data['volatility_20d'] = data['return_1d'].rolling(20).std() * np.sqrt(252)\n",
        "    \n",
        "    data['sma_20'] = data['Close'].rolling(20).mean()\n",
        "    data['sma_50'] = data['Close'].rolling(50).mean()\n",
        "    data['close_to_sma_20'] = (data['Close'] - data['sma_20']) / data['sma_20']\n",
        "    data['close_to_sma_50'] = (data['Close'] - data['sma_50']) / data['sma_50']\n",
        "    \n",
        "    # RSI\n",
        "    delta = data['Close'].diff()\n",
        "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
        "    rs = gain / (loss + 1e-10)\n",
        "    data['rsi_14_norm'] = (100 - (100 / (1 + rs)) - 50) / 50\n",
        "    \n",
        "    # MACD\n",
        "    ema_12 = data['Close'].ewm(span=12).mean()\n",
        "    ema_26 = data['Close'].ewm(span=26).mean()\n",
        "    macd = ema_12 - ema_26\n",
        "    macd_signal = macd.ewm(span=9).mean()\n",
        "    data['macd_histogram_norm'] = (macd - macd_signal) / data['Close']\n",
        "    \n",
        "    # Bollinger Bands\n",
        "    bb_mid = data['Close'].rolling(20).mean()\n",
        "    bb_std = data['Close'].rolling(20).std()\n",
        "    bb_upper = bb_mid + 2 * bb_std\n",
        "    bb_lower = bb_mid - 2 * bb_std\n",
        "    data['bb_position'] = (data['Close'] - bb_lower) / (bb_upper - bb_lower + 1e-10)\n",
        "    \n",
        "    # === SMC/ICT (15) ===\n",
        "    data['swing_high'] = data['High'].rolling(10, center=True).max()\n",
        "    data['swing_low'] = data['Low'].rolling(10, center=True).min()\n",
        "    \n",
        "    data['prev_swing_high'] = data['swing_high'].shift(10)\n",
        "    data['prev_swing_low'] = data['swing_low'].shift(10)\n",
        "    data['higher_high'] = (data['swing_high'] > data['prev_swing_high']).astype(int)\n",
        "    data['higher_low'] = (data['swing_low'] > data['prev_swing_low']).astype(int)\n",
        "    data['lower_high'] = (data['swing_high'] < data['prev_swing_high']).astype(int)\n",
        "    data['lower_low'] = (data['swing_low'] < data['prev_swing_low']).astype(int)\n",
        "    \n",
        "    data['structure_score'] = (data['higher_high'].rolling(5).sum() + \n",
        "                               data['higher_low'].rolling(5).sum() -\n",
        "                               data['lower_high'].rolling(5).sum() - \n",
        "                               data['lower_low'].rolling(5).sum()) / 10\n",
        "    \n",
        "    range_high = data['High'].rolling(50).max()\n",
        "    range_low = data['Low'].rolling(50).min()\n",
        "    data['range_position'] = (data['Close'] - range_low) / (range_high - range_low + 1e-10)\n",
        "    \n",
        "    data['dist_to_swing_high'] = (data['swing_high'] - data['Close']) / data['Close']\n",
        "    data['dist_to_swing_low'] = (data['Close'] - data['swing_low']) / data['Close']\n",
        "    \n",
        "    data['in_discount'] = (data['range_position'] < 0.5).astype(int)\n",
        "    data['in_deep_discount'] = (data['range_position'] < 0.3).astype(int)\n",
        "    data['in_premium'] = (data['range_position'] > 0.7).astype(int)\n",
        "    \n",
        "    vol_threshold = data['return_1d'].rolling(20).std() * 2\n",
        "    data['near_bullish_ob'] = (data['return_1d'] > vol_threshold).rolling(10).sum()\n",
        "    data['near_bearish_ob'] = (data['return_1d'] < -vol_threshold).rolling(10).sum()\n",
        "    \n",
        "    data['gap_up'] = ((data['Low'] > data['High'].shift(1)) & (data['return_1d'] > 0.01)).astype(int)\n",
        "    data['gap_down'] = ((data['High'] < data['Low'].shift(1)) & (data['return_1d'] < -0.01)).astype(int)\n",
        "    data['bullish_fvg'] = data['gap_up'].rolling(5).sum()\n",
        "    data['bearish_fvg'] = data['gap_down'].rolling(5).sum()\n",
        "    \n",
        "    data['sweep_high'] = ((data['High'] > data['swing_high'].shift(1)) & \n",
        "                          (data['Close'] < data['swing_high'].shift(1))).astype(int)\n",
        "    data['sweep_low'] = ((data['Low'] < data['swing_low'].shift(1)) & \n",
        "                         (data['Close'] > data['swing_low'].shift(1))).astype(int)\n",
        "    \n",
        "    data['bos_bullish'] = ((data['Close'] > data['swing_high'].shift(1)) & \n",
        "                           (data['higher_high'] == 1)).astype(int)\n",
        "    data['bos_bearish'] = ((data['Close'] < data['swing_low'].shift(1)) & \n",
        "                           (data['lower_low'] == 1)).astype(int)\n",
        "    \n",
        "    # === VOLUME (8) ===\n",
        "    data['volume_ma_20'] = data['Volume'].rolling(20).mean()\n",
        "    data['volume_ratio'] = data['Volume'] / (data['volume_ma_20'] + 1e-10)\n",
        "    data['volume_trend'] = data['Volume'].rolling(5).mean() / (data['Volume'].rolling(20).mean() + 1e-10)\n",
        "    \n",
        "    obv = (np.sign(data['Close'].diff()) * data['Volume']).cumsum()\n",
        "    data['obv_slope'] = obv.diff(5) / (obv.rolling(20).std() + 1e-10)\n",
        "    \n",
        "    data['vwap'] = (data['Close'] * data['Volume']).cumsum() / data['Volume'].cumsum()\n",
        "    data['close_to_vwap'] = (data['Close'] - data['vwap']) / (data['vwap'] + 1e-10)\n",
        "    \n",
        "    data['buying_pressure'] = (data['Close'] - data['Low']) / (data['High'] - data['Low'] + 1e-10)\n",
        "    data['accumulation_score'] = (data['buying_pressure'] * data['volume_ratio']).rolling(5).mean()\n",
        "    data['big_volume_day'] = (data['volume_ratio'] > 2).astype(int)\n",
        "    \n",
        "    # === MULTI-TIMEFRAME (7) ===\n",
        "    data['daily_trend'] = (data['Close'] > data['sma_20']).astype(int)\n",
        "    \n",
        "    data['weekly_close'] = data['Close'].rolling(5).mean()\n",
        "    data['weekly_high'] = data['High'].rolling(5).max()\n",
        "    data['weekly_low'] = data['Low'].rolling(5).min()\n",
        "    data['weekly_trend'] = (data['weekly_close'] > data['weekly_close'].shift(5)).astype(int)\n",
        "    \n",
        "    data['monthly_close'] = data['Close'].rolling(21).mean()\n",
        "    data['monthly_trend'] = (data['monthly_close'] > data['monthly_close'].shift(21)).astype(int)\n",
        "    \n",
        "    data['mtf_alignment'] = (data['daily_trend'] + data['weekly_trend'] + data['monthly_trend']) / 3\n",
        "    \n",
        "    data['weekly_range_pos'] = (data['Close'] - data['weekly_low']) / (data['weekly_high'] - data['weekly_low'] + 1e-10)\n",
        "    data['monthly_range_pos'] = (data['Close'] - data['Low'].rolling(21).min()) / \\\n",
        "                                (data['High'].rolling(21).max() - data['Low'].rolling(21).min() + 1e-10)\n",
        "    \n",
        "    data['trend_strength'] = abs(data['close_to_sma_20']) + abs(data['close_to_sma_50'])\n",
        "    \n",
        "    # Select and clean\n",
        "    features_df = data[FEATURE_NAMES].copy()\n",
        "    features_df = features_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    return features_df\n",
        "\n",
        "def create_labels(df):\n",
        "    \"\"\"Create labels: 0=SHORT, 1=NEUTRAL, 2=LONG\"\"\"\n",
        "    forward_return = df['Close'].shift(-config.PREDICTION_HORIZON) / df['Close'] - 1\n",
        "    labels = pd.Series(1, index=df.index)\n",
        "    labels[forward_return >= config.UP_THRESHOLD] = 2   # LONG\n",
        "    labels[forward_return <= config.DOWN_THRESHOLD] = 0  # SHORT\n",
        "    return labels\n",
        "\n",
        "# Calculate features for all stocks\n",
        "print(\"üîß Calculating features...\")\n",
        "feature_data = {}\n",
        "labels_data = {}\n",
        "\n",
        "for symbol, data in stock_data.items():\n",
        "    try:\n",
        "        features = calculate_features(data)\n",
        "        labels = create_labels(data)\n",
        "        if len(features) > config.LOOKBACK_DAYS + config.PREDICTION_HORIZON:\n",
        "            feature_data[symbol] = features\n",
        "            labels_data[symbol] = labels\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(f\"‚úÖ Processed {len(feature_data)} stocks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 5: Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SwingDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return {'features': self.features[idx], 'labels': self.labels[idx]}\n",
        "\n",
        "# Create sequences\n",
        "print(\"üìä Creating sequences...\")\n",
        "all_sequences, all_labels, all_dates = [], [], []\n",
        "\n",
        "for symbol in feature_data.keys():\n",
        "    features = feature_data[symbol].values\n",
        "    labels = labels_data[symbol].values\n",
        "    dates = feature_data[symbol].index\n",
        "    \n",
        "    for i in range(config.LOOKBACK_DAYS, len(features) - config.PREDICTION_HORIZON):\n",
        "        seq = features[i-config.LOOKBACK_DAYS:i]\n",
        "        label = labels[i]\n",
        "        if not np.isnan(label) and not np.any(np.isnan(seq)):\n",
        "            all_sequences.append(seq)\n",
        "            all_labels.append(int(label))\n",
        "            all_dates.append(dates[i])\n",
        "\n",
        "sequences = np.array(all_sequences)\n",
        "labels = np.array(all_labels)\n",
        "dates_arr = np.array(all_dates)\n",
        "\n",
        "print(f\"‚úÖ Total sequences: {len(sequences):,}\")\n",
        "print(f\"   Shape: {sequences.shape}\")\n",
        "\n",
        "# Split by time\n",
        "train_end = pd.Timestamp(config.TRAIN_END)\n",
        "val_end = pd.Timestamp(config.VAL_END)\n",
        "\n",
        "train_mask = dates_arr <= train_end\n",
        "val_mask = (dates_arr > train_end) & (dates_arr <= val_end)\n",
        "test_mask = dates_arr > val_end\n",
        "\n",
        "train_dataset = SwingDataset(sequences[train_mask], labels[train_mask])\n",
        "val_dataset = SwingDataset(sequences[val_mask], labels[val_mask])\n",
        "test_dataset = SwingDataset(sequences[test_mask], labels[test_mask])\n",
        "\n",
        "print(f\"\\nüìä Dataset Splits:\")\n",
        "print(f\"   Train: {len(train_dataset):,}\")\n",
        "print(f\"   Val:   {len(val_dataset):,}\")\n",
        "print(f\"   Test:  {len(test_dataset):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Step 6: Train CatBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üöÄ TRAINING CATBOOST MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare data (use last day features for CatBoost)\n",
        "X_train = train_dataset.features[:, -1, :].numpy()\n",
        "y_train = train_dataset.labels.numpy()\n",
        "X_val = val_dataset.features[:, -1, :].numpy()\n",
        "y_val = val_dataset.labels.numpy()\n",
        "\n",
        "print(f\"Training samples: {len(X_train):,}\")\n",
        "print(f\"Features: {X_train.shape[1]}\")\n",
        "\n",
        "# Train CatBoost\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=config.CATBOOST_ITERATIONS,\n",
        "    depth=config.CATBOOST_DEPTH,\n",
        "    learning_rate=config.CATBOOST_LR,\n",
        "    loss_function='MultiClass',\n",
        "    eval_metric='Accuracy',\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        "    early_stopping_rounds=50,\n",
        "    task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
        "    class_weights={0: 1.2, 1: 0.8, 2: 1.0}\n",
        ")\n",
        "\n",
        "catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
        "\n",
        "cat_val_acc = (catboost_model.predict(X_val) == y_val).mean()\n",
        "print(f\"\\n‚úÖ CatBoost Val Accuracy: {cat_val_acc:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "importance = dict(zip(FEATURE_NAMES, catboost_model.feature_importances_))\n",
        "sorted_imp = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nüìä Top 10 Important Features:\")\n",
        "for i, (f, v) in enumerate(sorted_imp[:10]):\n",
        "    print(f\"   {i+1}. {f}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Step 7: Train TFT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TFTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        h, heads, drop = config.TFT_HIDDEN_SIZE, config.TFT_ATTENTION_HEADS, config.TFT_DROPOUT\n",
        "        \n",
        "        self.var_sel = nn.Sequential(nn.Linear(40, h), nn.ReLU(), nn.Dropout(drop), nn.Linear(h, 40), nn.Softmax(dim=-1))\n",
        "        self.lstm = nn.LSTM(40, h, 2, batch_first=True, dropout=drop)\n",
        "        self.attention = nn.MultiheadAttention(h, heads, dropout=drop, batch_first=True)\n",
        "        self.grn = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.Dropout(drop), nn.Linear(h, h))\n",
        "        self.grn_gate = nn.Sequential(nn.Linear(h, h), nn.Sigmoid())\n",
        "        self.grn_norm = nn.LayerNorm(h)\n",
        "        self.output = nn.Linear(h, 3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        w = self.var_sel(x.mean(1))\n",
        "        x = x * w.unsqueeze(1)\n",
        "        o, _ = self.lstm(x)\n",
        "        a, _ = self.attention(o, o, o)\n",
        "        g = self.grn_gate(a) * self.grn(a)\n",
        "        out = self.grn_norm(a + g)\n",
        "        return self.output(out[:, -1, :])\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ TRAINING TFT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "tft_model = TFTModel().to(DEVICE)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE)\n",
        "optimizer = torch.optim.AdamW(tft_model.parameters(), lr=config.LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc, patience_count, best_state = 0, 0, None\n",
        "\n",
        "for epoch in range(config.MAX_EPOCHS):\n",
        "    # Train\n",
        "    tft_model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = tft_model(batch['features'].to(DEVICE))\n",
        "        loss = criterion(out, batch['labels'].to(DEVICE))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(tft_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validate\n",
        "    tft_model.eval()\n",
        "    correct = sum((tft_model(b['features'].to(DEVICE)).argmax(1) == b['labels'].to(DEVICE)).sum().item() \n",
        "                  for b in val_loader)\n",
        "    acc = correct / len(val_dataset)\n",
        "    print(f\"Epoch {epoch+1}: Val Acc = {acc:.4f}\")\n",
        "    \n",
        "    if acc > best_acc:\n",
        "        best_acc, patience_count = acc, 0\n",
        "        best_state = {k: v.cpu().clone() for k, v in tft_model.state_dict().items()}\n",
        "    else:\n",
        "        patience_count += 1\n",
        "        if patience_count >= config.PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "tft_model.load_state_dict(best_state)\n",
        "print(f\"\\n‚úÖ TFT Best Val Accuracy: {best_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Step 8: Train Stockformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StockformerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        d, h, l = config.STOCKFORMER_D_MODEL, config.STOCKFORMER_N_HEADS, config.STOCKFORMER_N_LAYERS\n",
        "        \n",
        "        self.trend_enc = nn.Sequential(nn.Linear(40, d), nn.LayerNorm(d), nn.GELU())\n",
        "        self.seasonal_enc = nn.Sequential(nn.Linear(40, d), nn.LayerNorm(d), nn.GELU())\n",
        "        self.residual_enc = nn.Sequential(nn.Linear(40, d), nn.LayerNorm(d), nn.GELU())\n",
        "        \n",
        "        self.pos = nn.Parameter(torch.randn(1, 60, d) * 0.02)\n",
        "        \n",
        "        enc = nn.TransformerEncoderLayer(d, h, d*4, 0.1, batch_first=True)\n",
        "        self.trend_tf = nn.TransformerEncoder(enc, l)\n",
        "        self.seasonal_tf = nn.TransformerEncoder(nn.TransformerEncoderLayer(d, h, d*4, 0.1, batch_first=True), l)\n",
        "        self.residual_tf = nn.TransformerEncoder(nn.TransformerEncoderLayer(d, h, d*4, 0.1, batch_first=True), l)\n",
        "        \n",
        "        self.fusion = nn.Sequential(nn.Linear(d*3, d*2), nn.GELU(), nn.Linear(d*2, d))\n",
        "        self.output = nn.Linear(d, 3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, s, _ = x.shape\n",
        "        trend = x.cumsum(1) / torch.arange(1, s+1, device=x.device).view(1, -1, 1)\n",
        "        seasonal = x - trend\n",
        "        \n",
        "        t = self.trend_tf(self.trend_enc(trend) + self.pos[:, :s])[:, -1]\n",
        "        se = self.seasonal_tf(self.seasonal_enc(seasonal) + self.pos[:, :s])[:, -1]\n",
        "        r = self.residual_tf(self.residual_enc(x) + self.pos[:, :s])[:, -1]\n",
        "        \n",
        "        return self.output(self.fusion(torch.cat([t, se, r], -1)))\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ TRAINING STOCKFORMER MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sf_model = StockformerModel().to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(sf_model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "best_acc, patience_count, best_state = 0, 0, None\n",
        "\n",
        "for epoch in range(config.MAX_EPOCHS):\n",
        "    # Train\n",
        "    sf_model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = sf_model(batch['features'].to(DEVICE))\n",
        "        loss = criterion(out, batch['labels'].to(DEVICE))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(sf_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validate\n",
        "    sf_model.eval()\n",
        "    correct = sum((sf_model(b['features'].to(DEVICE)).argmax(1) == b['labels'].to(DEVICE)).sum().item() \n",
        "                  for b in val_loader)\n",
        "    acc = correct / len(val_dataset)\n",
        "    print(f\"Epoch {epoch+1}: Val Acc = {acc:.4f}\")\n",
        "    \n",
        "    if acc > best_acc:\n",
        "        best_acc, patience_count = acc, 0\n",
        "        best_state = {k: v.cpu().clone() for k, v in sf_model.state_dict().items()}\n",
        "    else:\n",
        "        patience_count += 1\n",
        "        if patience_count >= config.PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "sf_model.load_state_dict(best_state)\n",
        "print(f\"\\n‚úÖ Stockformer Best Val Accuracy: {best_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 9: Ensemble Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üìä ENSEMBLE EVALUATION ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get test data\n",
        "X_test = test_dataset.features.numpy()\n",
        "y_test = test_dataset.labels.numpy()\n",
        "\n",
        "# CatBoost predictions\n",
        "cat_probs = catboost_model.predict_proba(X_test[:, -1, :])\n",
        "\n",
        "# TFT predictions\n",
        "tft_model.eval()\n",
        "with torch.no_grad():\n",
        "    tft_probs = torch.softmax(tft_model(torch.FloatTensor(X_test).to(DEVICE)), 1).cpu().numpy()\n",
        "\n",
        "# Stockformer predictions\n",
        "sf_model.eval()\n",
        "with torch.no_grad():\n",
        "    sf_probs = torch.softmax(sf_model(torch.FloatTensor(X_test).to(DEVICE)), 1).cpu().numpy()\n",
        "\n",
        "# Ensemble (weighted average)\n",
        "ensemble_probs = (config.CATBOOST_WEIGHT * cat_probs + \n",
        "                  config.TFT_WEIGHT * tft_probs + \n",
        "                  config.STOCKFORMER_WEIGHT * sf_probs)\n",
        "\n",
        "ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
        "ensemble_conf = np.max(ensemble_probs, axis=1)\n",
        "ensemble_acc = (ensemble_preds == y_test).mean()\n",
        "\n",
        "print(f\"\\nüéØ ENSEMBLE TEST ACCURACY: {ensemble_acc:.4f} ({ensemble_acc*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìã Individual Model Accuracy:\")\n",
        "print(f\"   CatBoost:    {(np.argmax(cat_probs, 1) == y_test).mean():.4f}\")\n",
        "print(f\"   TFT:         {(np.argmax(tft_probs, 1) == y_test).mean():.4f}\")\n",
        "print(f\"   Stockformer: {(np.argmax(sf_probs, 1) == y_test).mean():.4f}\")\n",
        "\n",
        "# Model agreement analysis\n",
        "cat_preds = np.argmax(cat_probs, axis=1)\n",
        "tft_preds = np.argmax(tft_probs, axis=1)\n",
        "sf_preds = np.argmax(sf_probs, axis=1)\n",
        "all_preds = np.stack([cat_preds, tft_preds, sf_preds], axis=1)\n",
        "agreements = (all_preds == ensemble_preds[:, None]).sum(axis=1)\n",
        "\n",
        "print(f\"\\nü§ù Agreement Analysis:\")\n",
        "for i in [1, 2, 3]:\n",
        "    mask = agreements == i\n",
        "    if mask.sum() > 0:\n",
        "        acc = (ensemble_preds[mask] == y_test[mask]).mean()\n",
        "        print(f\"   {i}/3 agree: {mask.sum():,} samples, accuracy: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nüìà Confidence Analysis:\")\n",
        "for thresh in [60, 70, 80]:\n",
        "    mask = ensemble_conf * 100 >= thresh\n",
        "    if mask.sum() > 0:\n",
        "        acc = (ensemble_preds[mask] == y_test[mask]).mean()\n",
        "        print(f\"   Conf >= {thresh}%: {mask.sum():,} ({mask.mean()*100:.1f}%), accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(classification_report(y_test, ensemble_preds, target_names=['SHORT', 'NEUTRAL', 'LONG']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 10: Save Models to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(config.MODEL_SAVE_PATH, exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "print(\"üíæ Saving models...\")\n",
        "\n",
        "# Save CatBoost\n",
        "catboost_model.save_model(f\"{config.MODEL_SAVE_PATH}catboost_model.cbm\")\n",
        "\n",
        "# Save TFT\n",
        "torch.save(tft_model.state_dict(), f\"{config.MODEL_SAVE_PATH}tft_model.pt\")\n",
        "\n",
        "# Save Stockformer\n",
        "torch.save(sf_model.state_dict(), f\"{config.MODEL_SAVE_PATH}stockformer_model.pt\")\n",
        "\n",
        "# Save config and features\n",
        "model_config = {\n",
        "    \"feature_columns\": FEATURE_NAMES,\n",
        "    \"num_features\": 40,\n",
        "    \"lookback_days\": config.LOOKBACK_DAYS,\n",
        "    \"ensemble_weights\": {\n",
        "        \"catboost\": config.CATBOOST_WEIGHT,\n",
        "        \"tft\": config.TFT_WEIGHT,\n",
        "        \"stockformer\": config.STOCKFORMER_WEIGHT\n",
        "    },\n",
        "    \"thresholds\": {\n",
        "        \"up\": config.UP_THRESHOLD,\n",
        "        \"down\": config.DOWN_THRESHOLD\n",
        "    },\n",
        "    \"test_accuracy\": float(ensemble_acc),\n",
        "    \"trained_at\": timestamp\n",
        "}\n",
        "\n",
        "with open(f\"{config.MODEL_SAVE_PATH}model_config.json\", \"w\") as f:\n",
        "    json.dump(model_config, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ All models saved to: {config.MODEL_SAVE_PATH}\")\n",
        "print(f\"   üìÅ catboost_model.cbm\")\n",
        "print(f\"   üìÅ tft_model.pt\")\n",
        "print(f\"   üìÅ stockformer_model.pt\")\n",
        "print(f\"   üìÅ model_config.json\")\n",
        "print(f\"\\nüéâ Training complete! Test accuracy: {ensemble_acc*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéâ Training Complete!\n",
        "\n",
        "Your models are saved to Google Drive. Download them and upload to Modal for production inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Next Steps:**\n",
        "1. Download models from Google Drive\n",
        "2. Upload to Modal volume using `ml/inference/modal_inference.py`\n",
        "3. Deploy backend to Railway\n",
        "4. Test signal generation!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
